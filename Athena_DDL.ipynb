{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ce0479-9fee-4516-826c-cabccaa162f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_APP: Spark Session UI - http://03205cdd01e3:4040\n"
     ]
    }
   ],
   "source": [
    "# Generate the SparkSession\n",
    "from lib.spark_session import get_spark_session\n",
    "\n",
    "spark = get_spark_session(\"Generate DDL\")\n",
    "print(\"SPARK_APP: Spark Session UI - \"+ spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22c1c8aa-8327-4668-949e-0aea4916f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DDL for edw_ld.dim_customer_ld\n",
      "CREATE EXTERNAL TABLE edw_ld.dim_customer_ld (\n",
      "\t customer_id string, \n",
      "\t name string, \n",
      "\t address string, \n",
      "\t city string, \n",
      "\t state string, \n",
      "\t zip_code string, \n",
      "\t phone_number string, \n",
      "\t email string, \n",
      "\t date_of_birth string, \n",
      "\t plan_type string, \n",
      "\t insert_dt timestamp, \n",
      "\t rundate string\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw_ld.db/dim_customer_ld/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw_ld.dim_date_ld\n",
      "CREATE EXTERNAL TABLE edw_ld.dim_date_ld (\n",
      "\t date string, \n",
      "\t day string, \n",
      "\t month string, \n",
      "\t year string, \n",
      "\t day_of_week string, \n",
      "\t insert_dt timestamp, \n",
      "\t rundate string\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw_ld.db/dim_date_ld/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw_ld.dim_product_ld\n",
      "CREATE EXTERNAL TABLE edw_ld.dim_product_ld (\n",
      "\t product_id string, \n",
      "\t product_name string, \n",
      "\t brand string, \n",
      "\t type string, \n",
      "\t flavor string, \n",
      "\t size string, \n",
      "\t price string, \n",
      "\t quantity string, \n",
      "\t expiration_date string, \n",
      "\t image_url string, \n",
      "\t insert_dt timestamp, \n",
      "\t rundate string\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw_ld.db/dim_product_ld/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw_ld.dim_sales_ld\n",
      "CREATE EXTERNAL TABLE edw_ld.dim_sales_ld (\n",
      "\t value string, \n",
      "\t insert_dt timestamp, \n",
      "\t rundate string\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw_ld.db/dim_sales_ld/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw_ld.dim_store_ld\n",
      "CREATE EXTERNAL TABLE edw_ld.dim_store_ld (\n",
      "\t store_id string, \n",
      "\t store_name string, \n",
      "\t address string, \n",
      "\t city string, \n",
      "\t state string, \n",
      "\t zip_code string, \n",
      "\t phone_number string, \n",
      "\t insert_dt timestamp, \n",
      "\t rundate string\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw_ld.db/dim_store_ld/_symlink_format_manifest/'\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "# Get all Tables in Landing Schema\n",
    "schema_name = 'edw_ld'\n",
    "table_list_df = spark.sql(f\"show tables in {schema_name}\")\n",
    "\n",
    "for table in table_list_df.collect():\n",
    "    table_name = f\"{schema_name}.{table['tableName']}\"\n",
    "    #print(table_name)\n",
    "    print(f\"-- DDL for {table_name}\")\n",
    "    df = spark.read.table(table_name)\n",
    "    cols:str = \"\"\n",
    "    for col in df.dtypes:\n",
    "        #print(col)\n",
    "        cols += f\"\\t {col[0]} {col[1]}, \\r\\n\"\n",
    "    #print(cols)\n",
    "    print(f\"CREATE EXTERNAL TABLE {table_name} (\")\n",
    "    print(cols[:-4])\n",
    "    print(\")\")\n",
    "    print(f\"\"\"ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/{schema_name}.db/{table['tableName']}/_symlink_format_manifest/'\n",
    ";\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "346816c3-ad69-4d07-a20d-63a4a85ec2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DDL for edw_stg.dim_customer_stg\n",
      "CREATE EXTERNAL TABLE edw_stg.dim_customer_stg (\n",
      "\t customer_id string, \n",
      "\t name string, \n",
      "\t address string, \n",
      "\t city string, \n",
      "\t state string, \n",
      "\t zip_code string, \n",
      "\t phone_number string, \n",
      "\t email string, \n",
      "\t date_of_birth date, \n",
      "\t plan_type string, \n",
      "\t insert_dt timestamp, \n",
      "\t rundate string, \n",
      "\t first_name string, \n",
      "\t last_name string, \n",
      "\t effective_start_date timestamp, \n",
      "\t effective_end_date timestamp, \n",
      "\t active_flag int, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw_stg.db/dim_customer_stg/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw_stg.dim_date_stg\n",
      "CREATE EXTERNAL TABLE edw_stg.dim_date_stg (\n",
      "\t date date, \n",
      "\t day int, \n",
      "\t month int, \n",
      "\t year int, \n",
      "\t day_of_week string, \n",
      "\t insert_dt timestamp, \n",
      "\t rundate string, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw_stg.db/dim_date_stg/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw_stg.dim_product_stg\n",
      "CREATE EXTERNAL TABLE edw_stg.dim_product_stg (\n",
      "\t product_id string, \n",
      "\t product_name string, \n",
      "\t brand string, \n",
      "\t type string, \n",
      "\t flavor string, \n",
      "\t size string, \n",
      "\t price double, \n",
      "\t image_url string, \n",
      "\t insert_dt timestamp, \n",
      "\t rundate string, \n",
      "\t expiration_dt date, \n",
      "\t effective_start_dt timestamp, \n",
      "\t effective_end_dt timestamp, \n",
      "\t active_flg int, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw_stg.db/dim_product_stg/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw_stg.dim_store_stg\n",
      "CREATE EXTERNAL TABLE edw_stg.dim_store_stg (\n",
      "\t store_id string, \n",
      "\t store_name string, \n",
      "\t address string, \n",
      "\t city string, \n",
      "\t state string, \n",
      "\t zip_code string, \n",
      "\t phone_number string, \n",
      "\t insert_dt timestamp, \n",
      "\t rundate string, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw_stg.db/dim_store_stg/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw_stg.fact_sales_stg\n",
      "CREATE EXTERNAL TABLE edw_stg.fact_sales_stg (\n",
      "\t cust_id string, \n",
      "\t store_id string, \n",
      "\t order_date string, \n",
      "\t qty int, \n",
      "\t tax double, \n",
      "\t discount double, \n",
      "\t line_total double, \n",
      "\t order_id string, \n",
      "\t invoice_num string, \n",
      "\t prod_id string, \n",
      "\t product_wid string, \n",
      "\t integration_key string, \n",
      "\t rundate string, \n",
      "\t insert_dt timestamp, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw_stg.db/fact_sales_stg/_symlink_format_manifest/'\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "# Get all Tables in Staging Schema\n",
    "schema_name = 'edw_stg'\n",
    "table_list_df = spark.sql(f\"show tables in {schema_name}\")\n",
    "\n",
    "for table in table_list_df.collect():\n",
    "    table_name = f\"{schema_name}.{table['tableName']}\"\n",
    "    #print(table_name)\n",
    "    print(f\"-- DDL for {table_name}\")\n",
    "    df = spark.read.table(table_name)\n",
    "    cols:str = \"\"\n",
    "    for col in df.dtypes:\n",
    "        #print(col)\n",
    "        cols += f\"\\t {col[0]} {col[1]}, \\r\\n\"\n",
    "    #print(cols)\n",
    "    print(f\"CREATE EXTERNAL TABLE {table_name} (\")\n",
    "    print(cols[:-4])\n",
    "    print(\")\")\n",
    "    print(f\"\"\"ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/{schema_name}.db/{table['tableName']}/_symlink_format_manifest/'\n",
    ";\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4012a05c-4835-4c81-8f2c-8fb59914cf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DDL for edw.dim_customer\n",
      "CREATE EXTERNAL TABLE edw.dim_customer (\n",
      "\t row_wid string, \n",
      "\t customer_id string, \n",
      "\t first_name string, \n",
      "\t last_name string, \n",
      "\t address string, \n",
      "\t city string, \n",
      "\t state string, \n",
      "\t zip_code string, \n",
      "\t phone_number string, \n",
      "\t email string, \n",
      "\t date_of_birth date, \n",
      "\t plan_type string, \n",
      "\t effective_start_date timestamp, \n",
      "\t effective_end_date timestamp, \n",
      "\t active_flag int, \n",
      "\t rundate string, \n",
      "\t insert_dt timestamp, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw.db/dim_customer/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw.dim_date\n",
      "CREATE EXTERNAL TABLE edw.dim_date (\n",
      "\t row_wid string, \n",
      "\t date date, \n",
      "\t day int, \n",
      "\t month int, \n",
      "\t year int, \n",
      "\t day_of_week string, \n",
      "\t rundate string, \n",
      "\t insert_dt timestamp, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw.db/dim_date/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw.dim_plan_type\n",
      "CREATE EXTERNAL TABLE edw.dim_plan_type (\n",
      "\t plan_type_code string, \n",
      "\t plan_name string, \n",
      "\t rundate string, \n",
      "\t insert_dt timestamp, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw.db/dim_plan_type/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw.dim_product\n",
      "CREATE EXTERNAL TABLE edw.dim_product (\n",
      "\t row_wid string, \n",
      "\t product_id string, \n",
      "\t product_name string, \n",
      "\t brand string, \n",
      "\t type string, \n",
      "\t flavor string, \n",
      "\t size string, \n",
      "\t price double, \n",
      "\t expiration_dt date, \n",
      "\t image_url string, \n",
      "\t effective_start_dt timestamp, \n",
      "\t effective_end_dt timestamp, \n",
      "\t active_flg int, \n",
      "\t rundate string, \n",
      "\t insert_dt timestamp, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw.db/dim_product/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw.dim_store\n",
      "CREATE EXTERNAL TABLE edw.dim_store (\n",
      "\t row_wid string, \n",
      "\t store_id string, \n",
      "\t store_name string, \n",
      "\t address string, \n",
      "\t city string, \n",
      "\t state string, \n",
      "\t zip_code string, \n",
      "\t phone_number string, \n",
      "\t rundate string, \n",
      "\t insert_dt timestamp, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw.db/dim_store/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw.fact_sales\n",
      "CREATE EXTERNAL TABLE edw.fact_sales (\n",
      "\t date_wid string, \n",
      "\t product_wid string, \n",
      "\t store_wid string, \n",
      "\t customer_wid string, \n",
      "\t order_id string, \n",
      "\t invoice_num string, \n",
      "\t qty int, \n",
      "\t tax double, \n",
      "\t discount double, \n",
      "\t line_total double, \n",
      "\t integration_key string, \n",
      "\t rundate string, \n",
      "\t insert_dt timestamp, \n",
      "\t update_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw.db/fact_sales/_symlink_format_manifest/'\n",
      ";\n",
      "-- DDL for edw.job_control\n",
      "CREATE EXTERNAL TABLE edw.job_control (\n",
      "\t schema_name string, \n",
      "\t table_name string, \n",
      "\t max_timestamp timestamp, \n",
      "\t rundate string, \n",
      "\t insert_dt timestamp\n",
      ")\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
      "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
      "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/edw.db/job_control/_symlink_format_manifest/'\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "# Get all Tables in Final Schema\n",
    "schema_name = 'edw'\n",
    "table_list_df = spark.sql(f\"show tables in {schema_name}\")\n",
    "\n",
    "for table in table_list_df.collect():\n",
    "    table_name = f\"{schema_name}.{table['tableName']}\"\n",
    "    #print(table_name)\n",
    "    print(f\"-- DDL for {table_name}\")\n",
    "    df = spark.read.table(table_name)\n",
    "    cols:str = \"\"\n",
    "    for col in df.dtypes:\n",
    "        #print(col)\n",
    "        cols += f\"\\t {col[0]} {col[1]}, \\r\\n\"\n",
    "    #print(cols)\n",
    "    print(f\"CREATE EXTERNAL TABLE {table_name} (\")\n",
    "    print(cols[:-4])\n",
    "    print(\")\")\n",
    "    print(f\"\"\"ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION 's3://deltalake12/dw-with-pyspark/warehouse/{schema_name}.db/{table['tableName']}/_symlink_format_manifest/'\n",
    ";\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc84ed-c0de-41cb-b347-81f01ae912ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
